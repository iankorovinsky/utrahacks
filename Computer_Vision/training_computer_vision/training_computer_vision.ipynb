{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### work in this repository was inspired by \n",
    "##### https://github.com/Kazuhito00/hand-gesture-recognition-using-mediapipe/blob/main/README_EN.md\n",
    "##### https://www.youtube.com/watch?v=a99p_fAr6e4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# this is just to make sure that the results are reproducible to anyone that runs the code lol.\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data formatting\n",
    "https://devqa.io/python-convert-csv-file-to-list/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "['data/train/start/start26.csv', 'data/train/start/start44.csv', 'data/train/start/start0.csv', 'data/train/start/start1.csv', 'data/train/start/start10.csv', 'data/train/start/start11.csv', 'data/train/start/start12.csv', 'data/train/start/start13.csv', 'data/train/start/start14.csv', 'data/train/start/start15.csv', 'data/train/start/start16.csv', 'data/train/start/start17.csv', 'data/train/start/start18.csv', 'data/train/start/start19.csv', 'data/train/start/start2.csv', 'data/train/start/start20.csv', 'data/train/start/start21.csv', 'data/train/start/start22.csv', 'data/train/start/start23.csv', 'data/train/start/start24.csv', 'data/train/start/start25.csv', 'data/train/start/start27.csv', 'data/train/start/start28.csv', 'data/train/start/start29.csv', 'data/train/start/start3.csv', 'data/train/start/start30.csv', 'data/train/start/start31.csv', 'data/train/start/start32.csv', 'data/train/start/start33.csv', 'data/train/start/start34.csv', 'data/train/start/start35.csv', 'data/train/start/start36.csv', 'data/train/start/start37.csv', 'data/train/start/start38.csv', 'data/train/start/start39.csv', 'data/train/start/start4.csv', 'data/train/start/start40.csv', 'data/train/start/start41.csv', 'data/train/start/start42.csv', 'data/train/start/start43.csv', 'data/train/start/start45.csv', 'data/train/start/start46.csv', 'data/train/start/start47.csv', 'data/train/start/start48.csv', 'data/train/start/start49.csv', 'data/train/start/start5.csv', 'data/train/start/start50.csv', 'data/train/start/start51.csv', 'data/train/start/start52.csv', 'data/train/start/start53.csv', 'data/train/start/start54.csv', 'data/train/start/start55.csv', 'data/train/start/start56.csv', 'data/train/start/start57.csv', 'data/train/start/start58.csv', 'data/train/start/start59.csv', 'data/train/start/start6.csv', 'data/train/start/start7.csv', 'data/train/start/start8.csv', 'data/train/start/start9.csv']\n",
      "60\n",
      "[218.47599029541016, 321.2573719024658, 201.79677963256836, 279.15693283081055, 212.06871032714844, 230.42837619781494, 228.4135627746582, 197.6455307006836, 231.85338973999023, 171.25462532043457, 260.26561737060547, 215.46100616455078, 274.827880859375, 177.4298858642578, 285.3101348876953, 155.1402711868286, 296.51636123657227, 137.72432327270508, 284.9685287475586, 229.32503700256348, 316.0531425476074, 194.67727661132812, 335.6250762939453, 173.0332374572754, 352.27794647216797, 155.8568572998047, 300.217227935791, 249.1355037689209, 333.4811782836914, 222.57554054260254, 352.3426818847656, 204.99325275421143, 368.25252532958984, 188.81452560424805, 308.0757141113281, 273.28471183776855, 339.2759323120117, 260.8761978149414, 359.7352600097656, 252.32128143310547, 377.67635345458984, 243.07233810424805]\n",
      "training set:\n",
      "1045\n",
      "1045\n",
      "turn\n",
      "['data/train/turn/turn26.csv', 'data/train/turn/turn44.csv', 'data/train/turn/turn0.csv', 'data/train/turn/turn1.csv', 'data/train/turn/turn10.csv', 'data/train/turn/turn11.csv', 'data/train/turn/turn12.csv', 'data/train/turn/turn13.csv', 'data/train/turn/turn14.csv', 'data/train/turn/turn15.csv', 'data/train/turn/turn16.csv', 'data/train/turn/turn17.csv', 'data/train/turn/turn18.csv', 'data/train/turn/turn19.csv', 'data/train/turn/turn2.csv', 'data/train/turn/turn20.csv', 'data/train/turn/turn21.csv', 'data/train/turn/turn22.csv', 'data/train/turn/turn23.csv', 'data/train/turn/turn24.csv', 'data/train/turn/turn25.csv', 'data/train/turn/turn27.csv', 'data/train/turn/turn28.csv', 'data/train/turn/turn29.csv', 'data/train/turn/turn3.csv', 'data/train/turn/turn30.csv', 'data/train/turn/turn31.csv', 'data/train/turn/turn32.csv', 'data/train/turn/turn33.csv', 'data/train/turn/turn34.csv', 'data/train/turn/turn35.csv', 'data/train/turn/turn36.csv', 'data/train/turn/turn37.csv', 'data/train/turn/turn38.csv', 'data/train/turn/turn39.csv', 'data/train/turn/turn4.csv', 'data/train/turn/turn40.csv', 'data/train/turn/turn41.csv', 'data/train/turn/turn42.csv', 'data/train/turn/turn43.csv', 'data/train/turn/turn45.csv', 'data/train/turn/turn46.csv', 'data/train/turn/turn47.csv', 'data/train/turn/turn48.csv', 'data/train/turn/turn49.csv', 'data/train/turn/turn5.csv', 'data/train/turn/turn50.csv', 'data/train/turn/turn51.csv', 'data/train/turn/turn52.csv', 'data/train/turn/turn53.csv', 'data/train/turn/turn54.csv', 'data/train/turn/turn55.csv', 'data/train/turn/turn56.csv', 'data/train/turn/turn57.csv', 'data/train/turn/turn58.csv', 'data/train/turn/turn59.csv', 'data/train/turn/turn6.csv', 'data/train/turn/turn7.csv', 'data/train/turn/turn8.csv', 'data/train/turn/turn9.csv']\n",
      "60\n",
      "[279.60174560546875, 263.76874923706055, 245.0604248046875, 256.99170112609863, 215.73675155639648, 230.05722999572754, 213.9126968383789, 197.6400089263916, 235.23239135742188, 190.21625518798828, 232.39931106567383, 204.30696487426758, 223.8076400756836, 185.65741539001465, 229.6152114868164, 209.56645488739014, 234.06299591064453, 215.93439102172852, 251.66122436523438, 200.36438941955566, 242.53662109375, 183.1774663925171, 246.92373275756836, 211.8261194229126, 251.796875, 214.66941833496094, 270.2850914001465, 199.30644035339355, 261.99520111083984, 182.11953163146973, 264.7813415527344, 211.65441513061523, 269.8749542236328, 216.56939506530762, 290.8381462097168, 199.5754909515381, 280.85460662841797, 187.25805759429932, 281.2198066711426, 208.05357456207275, 286.7642402648926, 213.23664665222168]\n",
      "training set:\n",
      "2232\n",
      "2232\n",
      "grapple\n",
      "['data/train/grapple/grapple24.csv', 'data/train/grapple/grapple40.csv', 'data/train/grapple/grapple0.csv', 'data/train/grapple/grapple1.csv', 'data/train/grapple/grapple10.csv', 'data/train/grapple/grapple11.csv', 'data/train/grapple/grapple12.csv', 'data/train/grapple/grapple13.csv', 'data/train/grapple/grapple14.csv', 'data/train/grapple/grapple15.csv', 'data/train/grapple/grapple16.csv', 'data/train/grapple/grapple17.csv', 'data/train/grapple/grapple18.csv', 'data/train/grapple/grapple19.csv', 'data/train/grapple/grapple2.csv', 'data/train/grapple/grapple20.csv', 'data/train/grapple/grapple21.csv', 'data/train/grapple/grapple22.csv', 'data/train/grapple/grapple23.csv', 'data/train/grapple/grapple25.csv', 'data/train/grapple/grapple26.csv', 'data/train/grapple/grapple27.csv', 'data/train/grapple/grapple28.csv', 'data/train/grapple/grapple29.csv', 'data/train/grapple/grapple3.csv', 'data/train/grapple/grapple30.csv', 'data/train/grapple/grapple31.csv', 'data/train/grapple/grapple32.csv', 'data/train/grapple/grapple33.csv', 'data/train/grapple/grapple34.csv', 'data/train/grapple/grapple35.csv', 'data/train/grapple/grapple36.csv', 'data/train/grapple/grapple37.csv', 'data/train/grapple/grapple38.csv', 'data/train/grapple/grapple39.csv', 'data/train/grapple/grapple4.csv', 'data/train/grapple/grapple41.csv', 'data/train/grapple/grapple42.csv', 'data/train/grapple/grapple43.csv', 'data/train/grapple/grapple44.csv', 'data/train/grapple/grapple45.csv', 'data/train/grapple/grapple46.csv', 'data/train/grapple/grapple47.csv', 'data/train/grapple/grapple48.csv', 'data/train/grapple/grapple49.csv', 'data/train/grapple/grapple5.csv', 'data/train/grapple/grapple50.csv', 'data/train/grapple/grapple51.csv', 'data/train/grapple/grapple52.csv', 'data/train/grapple/grapple53.csv', 'data/train/grapple/grapple54.csv', 'data/train/grapple/grapple55.csv', 'data/train/grapple/grapple56.csv', 'data/train/grapple/grapple57.csv', 'data/train/grapple/grapple58.csv', 'data/train/grapple/grapple59.csv', 'data/train/grapple/grapple6.csv', 'data/train/grapple/grapple7.csv', 'data/train/grapple/grapple8.csv', 'data/train/grapple/grapple9.csv']\n",
      "60\n",
      "[523.0784225463867, 445.8695983886719, 491.9159698486328, 411.30849838256836, 489.99401092529297, 358.9629364013672, 528.8862991333008, 329.9143409729004, 564.7311019897461, 322.9025173187256, 502.1125793457031, 302.07919120788574, 487.64110565185547, 246.26907348632812, 478.4663391113281, 212.6900339126587, 472.8984832763672, 182.89398193359375, 537.9697799682617, 301.32001876831055, 544.7367858886719, 238.221116065979, 550.5208969116211, 200.6710910797119, 557.3276901245117, 168.46293926239014, 565.3530120849609, 317.3664379119873, 588.4778594970703, 277.56382942199707, 578.7829971313477, 309.9401092529297, 569.8569107055664, 338.4822463989258, 585.7852554321289, 343.5926914215088, 595.5766677856445, 321.82886123657227, 582.4721527099609, 347.53698348999023, 572.3862838745117, 372.9672431945801]\n",
      "training set:\n",
      "2681\n",
      "2681\n",
      "start\n",
      "['data/test/start/start0.csv', 'data/test/start/start1.csv', 'data/test/start/start10.csv', 'data/test/start/start11.csv', 'data/test/start/start12.csv', 'data/test/start/start13.csv', 'data/test/start/start14.csv', 'data/test/start/start15.csv', 'data/test/start/start16.csv', 'data/test/start/start17.csv', 'data/test/start/start18.csv', 'data/test/start/start19.csv', 'data/test/start/start2.csv', 'data/test/start/start3.csv', 'data/test/start/start4.csv', 'data/test/start/start5.csv', 'data/test/start/start6.csv', 'data/test/start/start7.csv', 'data/test/start/start8.csv', 'data/test/start/start9.csv']\n",
      "20\n",
      "[199.9078369140625, 417.99665451049805, 160.78680038452148, 398.2474708557129, 132.04051971435547, 362.8988456726074, 113.22639465332031, 333.1591987609863, 93.55143547058105, 316.16432189941406, 161.02716445922852, 320.5876636505127, 143.92179489135742, 282.6318168640137, 133.02770614624023, 260.24831771850586, 124.84798431396484, 241.54343605041504, 186.40287399291992, 313.44703674316406, 181.43293380737305, 268.0399703979492, 177.7659797668457, 239.20509338378906, 175.48513412475586, 215.25133609771729, 209.02809143066406, 316.3128662109375, 211.5144920349121, 275.4481315612793, 212.76729583740234, 249.07648086547852, 213.20154190063477, 226.84813499450684, 229.52457427978516, 326.1486625671387, 246.00265502929688, 301.0513114929199, 256.6564750671387, 284.06956672668457, 265.2999687194824, 268.5326957702637]\n",
      "training set:\n",
      "288\n",
      "288\n",
      "turn\n",
      "['data/test/turn/turn27.csv', 'data/test/turn/turn41.csv', 'data/test/turn/turn42.csv', 'data/test/turn/turn43.csv', 'data/test/turn/turn44.csv', 'data/test/turn/turn45.csv', 'data/test/turn/turn46.csv', 'data/test/turn/turn47.csv', 'data/test/turn/turn48.csv', 'data/test/turn/turn49.csv', 'data/test/turn/turn50.csv', 'data/test/turn/turn51.csv', 'data/test/turn/turn52.csv', 'data/test/turn/turn53.csv', 'data/test/turn/turn54.csv', 'data/test/turn/turn55.csv', 'data/test/turn/turn56.csv', 'data/test/turn/turn57.csv', 'data/test/turn/turn58.csv', 'data/test/turn/turn59.csv']\n",
      "20\n",
      "[235.27971267700195, 407.3288154602051, 188.47759246826172, 387.6346778869629, 154.52136993408203, 354.5994758605957, 141.5087604522705, 320.45448303222656, 141.22715950012207, 288.8181495666504, 179.5400619506836, 310.8423328399658, 166.67861938476562, 284.5082187652588, 164.25418853759766, 313.37422370910645, 167.97252655029297, 340.06321907043457, 202.08637237548828, 308.5089683532715, 185.42606353759766, 282.6008605957031, 185.9343719482422, 318.89084815979004, 192.73401260375977, 346.7212200164795, 227.50938415527344, 309.83802795410156, 206.43321990966797, 282.18884468078613, 206.0459327697754, 319.27708625793457, 213.0295181274414, 348.0765438079834, 256.37332916259766, 316.3974094390869, 231.79798126220703, 297.6551055908203, 226.8906021118164, 323.6560535430908, 231.6636848449707, 346.0411262512207]\n",
      "training set:\n",
      "625\n",
      "625\n",
      "grapple\n",
      "['data/test/grapple/grapple0.csv', 'data/test/grapple/grapple1.csv', 'data/test/grapple/grapple10.csv', 'data/test/grapple/grapple11.csv', 'data/test/grapple/grapple12.csv', 'data/test/grapple/grapple13.csv', 'data/test/grapple/grapple14.csv', 'data/test/grapple/grapple15.csv', 'data/test/grapple/grapple16.csv', 'data/test/grapple/grapple17.csv', 'data/test/grapple/grapple18.csv', 'data/test/grapple/grapple19.csv', 'data/test/grapple/grapple2.csv', 'data/test/grapple/grapple3.csv', 'data/test/grapple/grapple4.csv', 'data/test/grapple/grapple5.csv', 'data/test/grapple/grapple6.csv', 'data/test/grapple/grapple7.csv', 'data/test/grapple/grapple8.csv', 'data/test/grapple/grapple9.csv']\n",
      "20\n",
      "[499.5969009399414, 370.4883670806885, 481.4578628540039, 337.8256416320801, 482.83260345458984, 296.5580177307129, 515.4885101318359, 272.74709701538086, 544.3595886230469, 263.67831230163574, 484.0217971801758, 261.49163246154785, 471.80397033691406, 220.13041019439697, 463.54381561279297, 195.19165992736816, 457.62054443359375, 174.27927017211914, 509.7914123535156, 263.92075538635254, 522.5543975830078, 216.9623851776123, 530.6150436401367, 189.42509651184082, 539.5200347900391, 166.1052417755127, 530.1497650146484, 277.89124488830566, 546.5873336791992, 258.140172958374, 534.3703460693359, 279.17750358581543, 526.2445068359375, 295.8584976196289, 545.4010009765625, 297.0326900482178, 554.5103454589844, 278.6026954650879, 545.9837341308594, 290.8545112609863, 540.9883117675781, 303.7998390197754]\n",
      "training set:\n",
      "1060\n",
      "1060\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "labels = {0: \"start\", 1: \"turn\", 2: \"grapple\"}\n",
    "\n",
    "Y_train = [] #1d array, (big_number, 1)\n",
    "X_train = [] #2d array, (big_number, 42)\n",
    "def data_append_train(data, X_train = X_train, Y_train = Y_train):\n",
    "  #formatting the labels, although I am not so sure what is required...\n",
    "  data1 = \"data/train\"\n",
    "  for label in labels: \n",
    "    name = labels[label]\n",
    "    path = f\"{data1}/{name}/*.csv\"\n",
    "    print(name)\n",
    "\n",
    "    # getting the directory\n",
    "    files = glob.glob(path)\n",
    "    print(files)\n",
    "    print(len(files))\n",
    "\n",
    "    # now concatenating the content of the files.\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "        \n",
    "            i = 0\n",
    "            for row in csv_reader:\n",
    "                #skip the header line\n",
    "                if i == 0: \n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                X_train.append([float(num) for num in row])\n",
    "                Y_train.append(label)\n",
    "    \n",
    "    print(X_train[-1])\n",
    "    print(\"training set:\")\n",
    "    print(len(X_train))\n",
    "    print(len(Y_train))    \n",
    "\n",
    "Y_test = []\n",
    "X_test = []\n",
    "def data_append_test(data, X_test = X_test, Y_test = Y_test):\n",
    "  #formatting the labels, although I am not so sure what is required...\n",
    "  data1 = \"data/test\"\n",
    "  for label in labels: \n",
    "    name = labels[label]\n",
    "    path = f\"{data1}/{name}/*.csv\"\n",
    "    print(name)\n",
    "\n",
    "    # getting the directory\n",
    "    files = glob.glob(path)\n",
    "    print(files)\n",
    "    print(len(files))\n",
    "\n",
    "    # now concatenating the content of the files.\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "        \n",
    "            i = 0\n",
    "            for row in csv_reader:\n",
    "                #skip the header line\n",
    "                if i == 0: \n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                X_test.append([float(num) for num in row])\n",
    "                Y_test.append(label)\n",
    "    \n",
    "    print(X_test[-1])\n",
    "    print(\"training set:\")\n",
    "    print(len(X_test))\n",
    "    print(len(Y_test))    \n",
    "#i'll only use the train test.\n",
    "data = \"data\"\n",
    "data_append_train(data)\n",
    "data_append_test(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### since I already have the training dataset split, I just need to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[509.31186676, 459.92548943, 461.01940155, ..., 295.18349648,\n",
       "         528.78093719, 274.66890335],\n",
       "        [375.90648651, 256.53282166, 348.931427  , ..., 156.07103348,\n",
       "         386.70227051, 142.1629715 ],\n",
       "        [404.26086426, 426.70277596, 352.19688416, ..., 224.50306892,\n",
       "         444.31091309, 199.56587791],\n",
       "        ...,\n",
       "        [ 72.45704651, 233.24028969,  34.21592236, ...,  91.3131237 ,\n",
       "         129.16862488,  72.83489943],\n",
       "        [259.07169342, 410.95962524, 228.1410408 , ..., 360.01324654,\n",
       "         271.52738571, 361.98019981],\n",
       "        [603.2503891 , 189.52393055, 594.64172363, ..., 168.2949543 ,\n",
       "         640.63529968, 171.43424034]]),\n",
       " array([0, 0, 0, ..., 0, 1, 1]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_df_train = pd.DataFrame(X_train)\n",
    "# Y_df_train = pd.DataFrame(Y_train)\n",
    "\n",
    "# X_df_test = pd.DataFrame(X_test)\n",
    "# Y_df_test = pd.DataFrame(Y_test)\n",
    "\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)\n",
    "# \n",
    "X_test = np.asarray(X_test)\n",
    "Y_test = np.asarray(Y_test)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# the random_state is so that both shuffle perform the same.\n",
    "X_train,Y_train = shuffle(X_train, Y_train, random_state=0)\n",
    "X_test,Y_test = shuffle(X_test, Y_test, random_state=0)\n",
    "# X_train.shape, Y_train.shape\n",
    "X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "# model_save_path = \"model/\"\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"model/model.{epoch:02d}-{val_accuracy:.2f}\",\n",
    "    monitor='val_accuracy',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    #everytime the accuracy gets better, it saves\n",
    "    save_freq='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Dense(units=32, activation='relu', input_shape=(42,)),\n",
    "    Dense(units=64, activation='relu',\n",
    "        kernel_regularizer=keras.regularizers.l1_l2(0.05)),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=16, activation='relu', \n",
    "        kernel_regularizer=keras.regularizers.l1_l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=NUM_CLASSES, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 9.4632 - accuracy: 0.4587"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Failed to format this callback filepath: \"model/model.{epoch:02d}-{val_accuracy:.2f}\". Reason: \\'val_accuracy\\''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#fitting\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/miniconda3/envs/ai_hackathon/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Programs/miniconda3/envs/ai_hackathon/lib/python3.9/site-packages/keras/src/callbacks.py:1615\u001b[0m, in \u001b[0;36mModelCheckpoint._get_file_path\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1611\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1612\u001b[0m             epoch\u001b[38;5;241m=\u001b[39mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, batch\u001b[38;5;241m=\u001b[39mbatch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlogs\n\u001b[1;32m   1613\u001b[0m         )\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1615\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1616\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to format this callback filepath: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1617\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReason: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1618\u001b[0m     )\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_filepath \u001b[38;5;241m=\u001b[39m distributed_file_utils\u001b[38;5;241m.\u001b[39mwrite_filepath(\n\u001b[1;32m   1620\u001b[0m     file_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdistribute_strategy\n\u001b[1;32m   1621\u001b[0m )\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_filepath\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Failed to format this callback filepath: \"model/model.{epoch:02d}-{val_accuracy:.2f}\". Reason: \\'val_accuracy\\''"
     ]
    }
   ],
   "source": [
    "#fitting\n",
    "history = model.fit(X_train,Y_train, epochs=50, batch_size=32, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
