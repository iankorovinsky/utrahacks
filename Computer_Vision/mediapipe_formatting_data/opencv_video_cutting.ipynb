{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 15:39:23.170384: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-20 15:39:23.215871: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-20 15:39:23.217451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-20 15:39:24.017344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#intial imports: \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "# import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import csv\n",
    "# from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "# import streamlit as st\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting up the necessary files for data transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_file(optional_int = 1):\n",
    "    rows = []\n",
    "    #creating empty file in folder, i added the start_time in the name of the csv file, so that if a symbol appears many times in a video, it will still be created in two different csv files, just that they will have different starting times\n",
    "    # csv_file = f\"/users/aly/documents/programming/apps/machine learning/asl converter/training_models/mediapipe/demo_test/demo.csv\"\n",
    "    # csv_file=\"d:/personnel/other learning/programming/personal_projects/asl_language_translation/training_models/mediapipe/demo_test/demo.csv\"\n",
    "    csv_file = f\"data/csv_file{optional_int}.csv\"\n",
    "    # if os.path.exists(csv_file):\n",
    "    #     return \n",
    "\n",
    "\n",
    "\n",
    "# Setup CSV File for the videos\n",
    "# 21 right hand landmarks, 21 left hand landmarks, 33 pose landmarks\n",
    "    num_coords = 21 + 21 #+ 33\n",
    "    landmarks = []\n",
    "    for val in range(1, num_coords+1):\n",
    "        landmarks += ['x{}'.format(val), 'y{}'.format(val)]#, 'z{}'.format(val), 'v{}'.format(val)]\n",
    "    print(\"Initialized an empty landmarks of size:\", len(landmarks))\n",
    "\n",
    "    with open(csv_file, mode='w', newline='') as f:\n",
    "        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csv_writer.writerow(landmarks)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting the hand coordinates: \n",
    "###### https://arkalsekar.medium.com/how-to-get-all-the-co-ordinates-of-hand-using-mediapipe-hand-solutions-ac7e2742f702\n",
    "###### https://www.futurelearn.com/info/courses/introduction-to-image-analysis-for-plant-phenotyping/0/steps/305359\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_cam():   \n",
    "  # For webcam input:\n",
    "  cap = cv2.VideoCapture(0)\n",
    "  with mp_hands.Hands(\n",
    "      min_detection_confidence=0.5,\n",
    "      min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "\n",
    "\n",
    "      success, image = cap.read()\n",
    "      if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        # If loading a video, use 'break' instead of 'continue'.\n",
    "        continue\n",
    "\n",
    "      # Flip the image horizontally for a later selfie-view display, and convert\n",
    "      # the BGR image to RGB.\n",
    "      image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "      # To improve performance, optionally mark the image as not writeable to\n",
    "      # pass by reference.\n",
    "      image.flags.writeable = False\n",
    "\n",
    "      #!results contains all the information about the image, in this case, we are looking at hands\n",
    "      results = hands.process(image)\n",
    "      \n",
    "      image_height, image_width, _ = image.shape\n",
    "      # Draw the hand annotations on the image.\n",
    "      image.flags.writeable = True\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "      #!checks for both hands, and looks if there is data\n",
    "      if results.multi_hand_landmarks:\n",
    "        #!extracting the information from the right hand\n",
    "        both_hand = results.multi_hand_landmarks.landmarks\n",
    "        hand_row = list(np.array([[landmark.x, landmark.y] for ids, landmark in both_hand]).flatten())\n",
    "\n",
    "\n",
    "        # for hand_landmarks in results.multi_hand_landmarks:\n",
    "        #   # Here is How to Get All the Coordinates\n",
    "        #   for ids, landmrk in enumerate(hand_landmarks.landmark):\n",
    "        #       # print(ids, landmrk)\n",
    "        #       cx, cy = landmrk.x * image_width, landmrk.y*image_height\n",
    "        #       # print(cx, cy)\n",
    "        #   print(enumerate(hand_landmarks.landmark))            # print (ids, cx, cy)\n",
    "        #   mp_drawing.draw_landmarks(\n",
    "        #       image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "      \n",
    "      # #!else, I wanna just write 0 for the information about the hands.\n",
    "      else: \n",
    "        \n",
    "        both_hand_row = list(np.array([[0,0,0,0] for i in range(42)]).flatten())\n",
    "\n",
    "      cv2.imshow('MediaPipe Hands', image)\n",
    "\n",
    "      #stop the process\n",
    "      if cv2.waitKey(5) & 0xFF == ord('q'): \n",
    "          break\n",
    "  # After the loop release the cap object \n",
    "  cap.release() \n",
    "\n",
    "  #closes all instance of the camera\n",
    "  cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized an empty landmarks of size: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libEGL warning: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/violet/Programs/miniconda3/envs/ai_hackathon/lib/python3.9/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'landmarks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m setup_file()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mopen_cam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mopen_cam\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#!checks for both hands, and looks if there is data\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[1;32m     33\u001b[0m   \u001b[38;5;66;03m#!extracting the information from the right hand\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m   both_hand \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_hand_landmarks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlandmarks\u001b[49m\n\u001b[1;32m     35\u001b[0m   hand_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39marray([[landmark\u001b[38;5;241m.\u001b[39mx, landmark\u001b[38;5;241m.\u001b[39my] \u001b[38;5;28;01mfor\u001b[39;00m ids, landmark \u001b[38;5;129;01min\u001b[39;00m both_hand])\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m     38\u001b[0m   \u001b[38;5;66;03m# for hand_landmarks in results.multi_hand_landmarks:\u001b[39;00m\n\u001b[1;32m     39\u001b[0m   \u001b[38;5;66;03m#   # Here is How to Get All the Coordinates\u001b[39;00m\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;66;03m#   for ids, landmrk in enumerate(hand_landmarks.landmark):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# #!else, I wanna just write 0 for the information about the hands.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'landmarks'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "setup_file()\n",
    "open_cam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the opencv library \n",
    "# import cv2 \n",
    "  \n",
    "  \n",
    "# # define a video capture object \n",
    "# vid = cv2.VideoCapture(0) \n",
    "  \n",
    "# while(True): \n",
    "      \n",
    "#     # Capture the video frame \n",
    "#     # by frame \n",
    "#     ret, frame = vid.read() \n",
    "  \n",
    "#     # Display the resulting frame \n",
    "#     cv2.imshow('frame', frame) \n",
    "\n",
    "#     #extracting the frames  \n",
    "    \n",
    "    \n",
    "\n",
    "#     # the 'q' button is set as the \n",
    "#     # quitting button you may use any \n",
    "#     # desired button of your choice \n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "#         break\n",
    "  \n",
    "# # After the loop release the cap object \n",
    "# vid.release() \n",
    "# # Destroy all the windows \n",
    "# cv2.destroyAllWindows() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
